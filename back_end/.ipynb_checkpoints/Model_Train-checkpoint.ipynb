{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assured-siemens",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-267c994c7e1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "import math\n",
    "from nltk.util import pr\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pattern3.en import singularize\n",
    "import re\n",
    "Size = 26709\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "strong-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification problem , either sarcastic or not sarcastic ,2 class problem\n",
    "alpha = ''\n",
    "Raw_Data = dict()\n",
    "X_headline =[]\n",
    "Y_Sarcastic=[]\n",
    "i=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "trained-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = []\n",
    "with open(\"Stopword-List.txt\",'r') as stop:\n",
    "    for line in stop:\n",
    "        temp = line.strip()\n",
    "        stop_word.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bridal-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Read data in X and Y labels ,headline and sarcacism score.\n",
    "# #and tokenize\n",
    "# #Tokenize\n",
    "# Tokens = []\n",
    "# count = 0\n",
    "# for lines in open('Sarcasm_Headlines_Dataset.json','r'):\n",
    "#     Raw_Data[i]=json.loads(lines)\n",
    "#     X_headline.append(Raw_Data[i][\"headline\"])\n",
    "#     Y_Sarcastic.append(Raw_Data[i][\"is_sarcastic\"])\n",
    "    \n",
    "#     #tokenize here\n",
    "#     for word in X_headline[i]:\n",
    "#         if(word == ' '):\n",
    "#             count += 1\n",
    "#             Tokens.append(alpha)\n",
    "#             alpha = ''\n",
    "#         else:\n",
    "#             alpha = alpha + word\n",
    "    \n",
    "#     i=i+1\n",
    "def cleaner(uncleaned):\n",
    "    porter = PorterStemmer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    uncleaned = uncleaned.lower()\n",
    "    uncleaned = uncleaned.strip()\n",
    "    uncleaned = uncleaned.translate({ord(i): None for i in '!\\\\@#-_:$%^&*();.,?/1”2’3“4‘567890\\'\\\"'})\n",
    "    for i in uncleaned:\n",
    "        t = ord(i)\n",
    "        if t < 97 or t>122:\n",
    "            uncleaned = uncleaned.replace(i, \"\")\n",
    "    uncleaned = porter.stem(uncleaned)\n",
    "    uncleaned = lemma.lemmatize(uncleaned, pos='v')\n",
    "    # uncleaned = singularize(uncleaned)\n",
    "    # uncleaned = re.sub(r'ly$', r'', uncleaned)\n",
    "    # uncleaned = re.sub(r'ed$', r'', uncleaned)\n",
    "    # uncleaned = re.sub(r'ing$', r'', uncleaned)\n",
    "    # uncleaned = re.sub(r'nes$', r'', uncleaned)\n",
    "    return uncleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "greatest-biology",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2f823f9f011c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sarcasm_Headlines_Dataset.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mRaw_Data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mX_headline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRaw_Data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"headline\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mY_Sarcastic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRaw_Data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"is_sarcastic\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "#Read data in X and Y labels ,headline and sarcacism score.\n",
    "#and tokenize\n",
    "#Tokenize\n",
    "Tokens = []\n",
    "count = 0\n",
    "lexicon = {}\n",
    "for lines in open('Sarcasm_Headlines_Dataset.json','r'):\n",
    "    Raw_Data[i]=json.loads(lines)\n",
    "    X_headline.append(Raw_Data[i][\"headline\"])\n",
    "    Y_Sarcastic.append(Raw_Data[i][\"is_sarcastic\"])\n",
    "    \n",
    "    #tokenize here\n",
    "    words = word_tokenize(X_headline[i])\n",
    "    curr = str(i)\n",
    "    for word in words:\n",
    "        if len(word) > 2:\n",
    "            if word in stop_word:\n",
    "                continue\n",
    "            word = cleaner(word)\n",
    "            Tokens.append(word)\n",
    "\n",
    "            if word in lexicon:\n",
    "                if curr in lexicon[word]:\n",
    "                    lexicon[word][i] += 1\n",
    "                else:\n",
    "                    lexicon[word][i] = 1\n",
    "            else:\n",
    "                lexicon[word] = {}\n",
    "                if curr in lexicon[word]:\n",
    "                    lexicon[word][i] += 1\n",
    "                else:\n",
    "                    lexicon[word][i] = 1\n",
    "    i=i+1\n",
    "    # print(word)\n",
    "    # for word in X_headline[i]:\n",
    "    #     if(word == ' '):\n",
    "    #         count += 1\n",
    "    #         Tokens.append(alpha)\n",
    "    #         alpha = ''\n",
    "    #     else:\n",
    "    #         alpha = alpha + word\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handled-saying",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word count is: 0\n",
      "Number of docs are: 0\n",
      "{}\n",
      "dictionary size is 0\n"
     ]
    }
   ],
   "source": [
    "#Tokenize\n",
    "# Tokens = []\n",
    "# count = 0\n",
    "# for y in range(len(X_headline)):\n",
    "#     for word in X_headline[y]:\n",
    "#         if(word == ' '):\n",
    "#             count += 1\n",
    "#             Tokens.append(alpha)\n",
    "#             alpha = ''\n",
    "#         else:\n",
    "#             alpha = alpha + word\n",
    "# print(Tokens)\n",
    "print(\"The word count is: \"+str(len(Tokens)))\n",
    "print(\"Number of docs are: \" + str(i))\n",
    "print(lexicon)\n",
    "print(\"dictionary size is\",len(lexicon))\n",
    "# print(type(Raw_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-ordinance",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "66cab2f9c265ece95b3064c3649bf2263af44ad45cce960d239f78ecc64793a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
