{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assured-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import json\n",
    "import math\n",
    "from nltk.util import pr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from joblib import dump, load\n",
    "Size = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# functions\n",
    "\n",
    "def make_idf(lexicon):\n",
    "    idf = dict()\n",
    "    global Size\n",
    "    N = Size\n",
    "    for term,docs in lexicon.items():\n",
    "        df = len(docs)\n",
    "        temp = math.log(N / df)\n",
    "        idf[term] = temp\n",
    "    return idf\n",
    "\n",
    "def cleaner(uncleaned):\n",
    "    porter = PorterStemmer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    uncleaned = uncleaned.lower()\n",
    "    uncleaned = uncleaned.strip()\n",
    "    uncleaned = uncleaned.translate(\n",
    "        {ord(i): None for i in '!\\\\@#-_:$%^&*();.,?/1”2’3“4‘567890\\'\\\"'})\n",
    "    for i in uncleaned:\n",
    "        t = ord(i)\n",
    "        if t < 97 or t > 122:\n",
    "            uncleaned = uncleaned.replace(i, \"\")\n",
    "    uncleaned = porter.stem(uncleaned)\n",
    "    uncleaned = lemma.lemmatize(uncleaned, pos='v')\n",
    "    return uncleaned\n",
    "\n",
    "def Read_Data_From_File():\n",
    "    Tokens = []\n",
    "    alpha = ''\n",
    "    Raw_Data = dict()\n",
    "    X_headline = []\n",
    "    Y_Sarcastic = []\n",
    "    i = 0\n",
    "    count = 0\n",
    "    lexicon = {}\n",
    "\n",
    "    stop_word = []\n",
    "    with open(\"Stopword-List.txt\", 'r') as stop:\n",
    "        for line in stop:\n",
    "            temp = line.strip()\n",
    "            stop_word.append(temp)\n",
    "\n",
    "    for lines in open('Sarcasm_Headlines_Dataset.json', 'r'):\n",
    "        count += 1\n",
    "        Raw_Data[i] = json.loads(lines)\n",
    "        X_headline.append(Raw_Data[i][\"headline\"])\n",
    "        Y_Sarcastic.append(Raw_Data[i][\"is_sarcastic\"])\n",
    "\n",
    "        #tokenize here\n",
    "        words = word_tokenize(X_headline[i])\n",
    "        curr = str(i)\n",
    "        for word in words:\n",
    "            if len(word) >= 3:\n",
    "                if word in stop_word:\n",
    "                    continue\n",
    "                taggs = nltk.tag.pos_tag([word])\n",
    "                if(taggs[0][1] != 'NNP' and taggs[0][1] != 'FW' and taggs[0][1] != 'PRP'):\n",
    "                    word = cleaner(word)\n",
    "                    if len(word) >= 3:                 \n",
    "                        # Tokens.append(word)\n",
    "\n",
    "                        if word in lexicon:\n",
    "                            if curr in lexicon[word]:\n",
    "                                lexicon[word][i] += 1\n",
    "                            else:\n",
    "                                lexicon[word][i] = 1\n",
    "                        else:\n",
    "                            lexicon[word] = {}\n",
    "                            if curr in lexicon[word]:\n",
    "                                lexicon[word][i] += 1\n",
    "                            else:\n",
    "                                lexicon[word][i] = 1\n",
    "        i = i+1\n",
    "        # print(lexicon)\n",
    "    global Size \n",
    "    Size = count\n",
    "    return lexicon\n",
    "\n",
    "def tf_idf_lexicon(lexicon, idf):\n",
    "    tf_idf = {}\n",
    "    for term, docs in lexicon.items():\n",
    "        tf_idf[term] = {}\n",
    "        for docNo, tf in docs.items():\n",
    "            tf_idf[term][docNo] = tf * idf[term]\n",
    "\n",
    "    return tf_idf\n",
    "\n",
    "def applyPCA(df):\n",
    "    print(\"In function\")\n",
    "    scaler = StandardScaler()\n",
    "    #scaler=MinMaxScaler()\n",
    "    print(\"Standard scaler defined\")\n",
    "    scaler.fit(df)\n",
    "    print(\"scaler Fit defined\")\n",
    "    scaled_data = scaler.transform(df)\n",
    "    print(\"scaler trasnform\")\n",
    "\n",
    "    pca = PCA(n_components=900)\n",
    "    pca.fit(scaled_data)\n",
    "    x_pca = pca.transform(scaled_data)\n",
    "    print(\"Data Variance: \", sum(pca.explained_variance_ratio_)*100)\n",
    "\n",
    "    return x_pca, scaled_data\n",
    "\n",
    "\n",
    "def sort(tfidf):\n",
    "    size = 26709\n",
    "    list1 = []\n",
    "    check = False\n",
    "    for i in range(size):\n",
    "        list1.append(i+1)\n",
    "    # print(list1)\n",
    "    \n",
    "    for word,doc in tfidf.items():\n",
    "        print(word)\n",
    "        for i in list1:\n",
    "            if i not in doc.keys():\n",
    "                tfidf[word][i] = 0\n",
    "    return tfidf\n",
    "\n",
    "def make_idf1(Tokens):\n",
    "    list1 = []\n",
    "    idf = dict()\n",
    "    global Size\n",
    "    N = Size\n",
    "    # for term,docs in lexicon.items():\n",
    "    #     df = len(docs)\n",
    "    #     temp = math.log(N / df)\n",
    "    #     idf[term] = temp\n",
    "    # return idf\n",
    "    for i in Tokens:\n",
    "        # print(i[0],i[1])        \n",
    "        count =  i[0]\n",
    "        while i[0]+1 == count:\n",
    "             list1.append(Tokens[i[1]])\n",
    "    print(list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lexicon\nIdf made\ntf_Idf made\n"
     ]
    }
   ],
   "source": [
    "lexicon = Read_Data_From_File()\n",
    "print(\"lexicon\")\n",
    "idf = make_idf(lexicon)\n",
    "print(\"Idf made\")\n",
    "tf_idf = tf_idf_lexicon(lexicon,idf)\n",
    "print(\"tf_Idf made\")\n",
    "# tf_idf = sort(tf_idf)\n",
    "with open(\"TFIDF_Dictionary.json\", \"w\") as f: # Writing the index to JSON File.\n",
    "   f.write(json.dumps(tf_idf, sort_keys=False, indent=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"IDF_Dictionary.json\", \"w\") as f: # Writing the index to JSON File.\n",
    "   f.write(json.dumps(idf, sort_keys=False, indent=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18616"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "len(idf)"
   ]
  },
  {
   "source": [
    "x_pca"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(26709, 18616)\n",
      "18616\n",
      "Data Processing Completed........\n",
      "\n",
      "Applying PCA.....................\n",
      "In function\n",
      "Standard scaler defined\n",
      "scaler Fit defined\n",
      "scaler trasnform\n",
      "Data Variance:  22.381134315944273\n",
      "[[-0.05438118 -0.0317108   0.0074228  ... -0.26486439  0.04284025\n",
      "  -1.01061468]\n",
      " [-0.04007727 -0.01453717 -0.03777587 ...  0.11131384  0.08171687\n",
      "  -0.34716208]\n",
      " [-0.08269532 -0.04001263 -0.02362679 ...  4.43565155 -2.92950651\n",
      "  -4.68224005]\n",
      " ...\n",
      " [-0.03373501 -0.01018237 -0.03598306 ... -0.24622564 -0.08508081\n",
      "  -0.0480576 ]\n",
      " [-0.06268075 -0.02089338  0.00949616 ... -2.27411292  1.34326509\n",
      "  -1.02793123]\n",
      " [-0.03056064 -0.00666135 -0.03710731 ... -0.12211445  0.19533831\n",
      "  -0.04966335]] [[ 1.59176452e+01  0.00000000e+00  2.49026010e+01 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " ...\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "   1.63425824e+02  0.00000000e+00]]\n",
      "\n",
      "PCA Applied......................\n",
      "\n",
      "Traning Model....................\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(26709, 18616)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('TFIDF_Dictionary.json') as json_file:\n",
    "    Dict = json.load(json_file)\n",
    "# Dict = sort(Dict)\n",
    "# print(Dict)\n",
    "# with open(\"TFIDF_Dictionary_new.json\", \"w\") as f:\n",
    "    # f.write(json.dumps(Dict, sort_keys=False, indent=4))\n",
    "\n",
    "#Create Panda DataFrame\n",
    "#    print(\"Hello\")\n",
    "df = pd.DataFrame(Dict.items())\n",
    "vocab = list(Dict.keys())\n",
    "vocab_len = len(vocab)\n",
    "word_to_inx = {}\n",
    "for word in vocab:\n",
    "    word_to_inx[word] = vocab.index(word)\n",
    "i = 0\n",
    "dataset = np.empty((26709, vocab_len))\n",
    "for line in open('Sarcasm_Headlines_Dataset.json', 'r'):\n",
    "    data = json.loads(line)\n",
    "    str_i = str(i)\n",
    "    docVector = np.zeros(vocab_len)\n",
    "    for word in data[\"headline\"].split(\" \"):\n",
    "        if word in word_to_inx and str_i in Dict[word]:\n",
    "            wordPos = word_to_inx[word]\n",
    "            docVector[wordPos] = Dict[word][str_i]\n",
    "            dataset[i] = docVector\n",
    "    i += 1\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "# df = np.transpose(df)\n",
    "# print(df[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset /= np.max(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.5434051 , 0.        , 0.60163443, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18616\n",
      "Data Processing Completed........\n",
      "\n",
      "Applying PCA.....................\n",
      "In function\n",
      "Standard scaler defined\n",
      "scaler Fit defined\n",
      "scaler trasnform\n"
     ]
    }
   ],
   "source": [
    "print(vocab_len)\n",
    "\n",
    "print(\"Data Processing Completed........\")\n",
    "print(\"\")\n",
    "print(\"Applying PCA.....................\")\n",
    "\n",
    "# x_pca, scaled_data = applyPCA(dataset)\n",
    "print(\"In function\")\n",
    "scaler = StandardScaler()\n",
    "#scaler=MinMaxScaler()\n",
    "print(\"Standard scaler defined\")\n",
    "scaler.fit(dataset)\n",
    "print(\"scaler Fit defined\")\n",
    "scaled_data = scaler.transform(dataset)\n",
    "print(\"scaler trasnform\")\n",
    "\n",
    "# pca = PCA(n_components=900)\n",
    "# pca.fit(scaled_data)\n",
    "# x_pca = pca.transform(scaled_data)\n",
    "# print(\"Data Variance: \", sum(pca.explained_variance_ratio_)*100)\n",
    "\n",
    "# print(x_pca,scaled_data)\n",
    "# print(\"\")\n",
    "# print(\"PCA Applied......................\")\n",
    "# print(\"\")\n",
    "# print(\"Traning Model....................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['pca.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# dump(pca, 'pca.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['scaler.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# dump(scaler, 'scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "Raw_Data = dict()\n",
    "X_headline = []\n",
    "Y_Sarcastic = []\n",
    "i = 0\n",
    "count = 0\n",
    "\n",
    "for lines in open('Sarcasm_Headlines_Dataset.json', 'r'):\n",
    "    count += 1\n",
    "    Raw_Data[i] = json.loads(lines)\n",
    "    X_headline.append(Raw_Data[i][\"headline\"])\n",
    "    Y_Sarcastic.append(Raw_Data[i][\"is_sarcastic\"])\n",
    "\n",
    "size=len(X_headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26709"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "len(Y_Sarcastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "# X_new = SelectKBest(mutual_info_classif, k=4000).fit_transform(dataset, Y_Sarcastic)\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "X_new = SelectKBest(chi2, k=4000).fit_transform(dataset, Y_Sarcastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Applying SVM.....................\n"
     ]
    }
   ],
   "source": [
    "#x_pca.to_csv('pData.csv')\n",
    "#x_pca=pd.read_csv('pData')\n",
    "\n",
    "#BreakData\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, Y_Sarcastic,test_size=0.30,random_state=48)\n",
    "\n",
    "print(\"Applying SVM.....................\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=18)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=18)\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6651691002121553"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "neigh.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Trained....................\n\nApplying Model...................\n"
     ]
    }
   ],
   "source": [
    "#Apply SVM\n",
    "model = svm.SVC(random_state=20)\n",
    "print(\"Model Trained....................\")\n",
    "print(\"\")\n",
    "print(\"Applying Model...................\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['model_70acc.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "\n",
    "# dump(model, 'model_70acc.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = load('model_70acc.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-33c23d1eb4de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0macc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy= \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "acc=model.score(X_test,y_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy= \",acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nAccuracy=  77.21720409978003 %\n"
     ]
    }
   ],
   "source": [
    "acc=model.score(X_train,y_train)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy= \",acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "model.predict([X_test[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('IDF_Dictionary.json') as json_file:\n",
    "    idf2 = json.load(json_file)\n",
    "vocab_len = len(idf2)\n",
    "query_dataset = np.empty((1, vocab_len))\n",
    "j = 0\n",
    "for i in query_dataset:\n",
    "    for x in i:\n",
    "        # print(x)\n",
    "        query_dataset[0][j] = 0\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"after careful consideration, bush recommends oil drilling\"\n",
    "tf_query = idf2.copy()\n",
    "for key in tf_query.keys():\n",
    "    tf_query[key] = 0.0\n",
    "words = word_tokenize(query)\n",
    "\n",
    "stop_word = []\n",
    "with open(\"Stopword-List.txt\", 'r') as stop:\n",
    "    for line in stop:\n",
    "        temp = line.strip()\n",
    "        stop_word.append(temp)\n",
    "\n",
    "for word in words:\n",
    "    if len(word) >= 3:\n",
    "        if word in stop_word:\n",
    "            continue\n",
    "        taggs = nltk.tag.pos_tag([word])\n",
    "        if(taggs[0][1] != 'NNP' and taggs[0][1] != 'FW' and taggs[0][1] != 'PRP'):\n",
    "            word = cleaner(word)\n",
    "            if len(word) >= 3:                 \n",
    "                # Tokens.append(word)\n",
    "                \n",
    "                if word in tf_query.keys():\n",
    "                    if tf_query[word] == 0:\n",
    "                        tf_query[word] = idf2[word]\n",
    "                    else:\n",
    "                        tf_query += idf2[word]\n",
    "# print(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in tf_query.values():\n",
    "    query_dataset[0][j] = i\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 18616)"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "query_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_q = load('pca.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Processing Completed........\n\nApplying PCA.....................\nIn function\nStandard scaler defined\nscaler Fit defined\nscaler trasnform\nData Variance:  22.381134315944273\n[[ 1.51939087e-15  2.59625184e-16  3.10229874e-15  3.62345360e-16\n   1.45243569e-16 -5.17010932e-16 -1.75750204e-15 -9.60684602e-16\n   3.07710810e-16  2.84391934e-16 -1.38842103e-16  1.27785708e-15\n   2.54894132e-15  2.84137885e-16 -1.57731741e-15 -2.14532250e-15\n   1.21496553e-15 -5.07656539e-16  1.71718332e-15 -1.76198725e-15\n   6.75226504e-16  8.26895453e-16 -5.32388192e-16 -1.68820230e-15\n  -3.99993796e-16  1.00120198e-16  8.80896592e-16 -1.28346174e-15\n   9.04670440e-16 -4.14883692e-16  1.64495553e-16  1.45658437e-15\n   2.35583178e-15  2.49476110e-15  1.02622506e-15  3.41311465e-16\n   5.64902206e-16 -2.57179624e-16  3.95657220e-16  2.71545153e-15\n   1.57584008e-15 -8.81835079e-16  8.83505040e-16  1.22598978e-15\n  -7.65177298e-16  1.42513730e-15  9.26072975e-17  6.55773366e-16\n   2.12582524e-15  6.47243680e-16  4.08848212e-16  8.69926982e-16\n   3.41445850e-15 -6.99687681e-16 -1.06979874e-15 -3.11178189e-16\n   1.13440716e-15  1.64038308e-15 -2.09034852e-15 -1.60246400e-15\n   3.58298381e-16  1.90119461e-15  3.00419161e-16  1.04375050e-15\n  -8.44811653e-16  3.99410144e-16  9.90272892e-16 -2.57252360e-15\n  -3.70162279e-16 -8.60594683e-16  1.04694395e-15 -1.38070437e-15\n  -3.88753148e-16 -1.06060353e-15  9.97070202e-16  2.01719864e-15\n  -1.37942154e-15 -5.60738205e-16 -7.80647963e-16  3.63999213e-17\n  -4.10722149e-16  9.25431065e-16  1.51044117e-15  1.51914055e-15\n  -2.06233070e-15 -1.09225681e-15 -1.01291645e-15  2.82177168e-16\n  -5.18608585e-16  2.96120535e-15 -1.63254291e-15 -6.14896486e-16\n   1.08671681e-15  8.23668858e-16 -4.26869900e-16  8.44374387e-16\n  -4.61918512e-16 -2.52231501e-17 -1.25654052e-15  2.42195611e-15\n   2.22990915e-16  4.70224929e-16 -5.71683653e-17 -4.17286151e-16\n   2.60451775e-15  2.65359715e-16  4.89062409e-16 -2.40405133e-15\n   2.62066964e-15 -1.30678475e-17 -1.50267698e-15 -5.75930580e-16\n   2.23783757e-16 -1.00596090e-16  1.39285699e-15  4.49412276e-17\n  -7.20463977e-17 -1.62858126e-15 -1.67200823e-15 -8.58036733e-16\n   1.67798575e-15  5.61258182e-16 -1.35781601e-15  4.67316118e-17\n   3.28828502e-16 -6.97218596e-16 -1.06390219e-15 -1.44634880e-15\n   1.74504213e-15 -2.89098884e-16  1.03195691e-15  1.34855518e-15\n   2.58696843e-16 -1.03632911e-15  9.58510989e-16 -1.63273836e-15\n   8.34623300e-16  5.16792819e-16 -2.71090281e-15  1.07419562e-15\n   2.49842314e-15 -2.86895227e-16 -2.98512953e-17 -1.06387164e-15\n  -2.59889884e-15  1.01154063e-15 -5.16421302e-16 -1.17206751e-15\n   2.25062466e-15 -2.18288420e-15 -8.85220807e-16 -8.75450699e-16\n  -2.63962509e-16 -8.75039327e-16  2.70423730e-16 -2.73197857e-16\n  -1.06248671e-15  7.24015658e-17 -1.99556907e-15 -8.65674862e-16\n   3.00248956e-17  1.04793243e-15  2.32918939e-15  2.17101570e-15\n   4.98170395e-16 -1.27457227e-15  1.14763446e-15  1.41609602e-15\n  -8.58272841e-16 -6.47309651e-16 -1.98054472e-15 -3.30354756e-16\n   2.19101007e-15 -1.78929096e-15 -2.59173670e-16  2.59306455e-16\n  -4.63272446e-17 -2.59480816e-16 -5.14566736e-16 -1.16038949e-15\n  -4.04648674e-16  1.82259381e-15 -6.82184393e-16  3.12243004e-16\n   1.61781438e-15 -9.41461377e-16  8.49706394e-16  1.67960807e-15\n  -1.94958655e-15  1.85732006e-15 -3.72106231e-16  9.24029851e-16\n  -1.16352928e-15 -6.49434271e-16 -1.69872533e-15  1.01392503e-15\n   1.00186672e-15 -9.68013499e-16  1.50766139e-15 -2.73596055e-16\n   1.53299543e-15 -7.23257830e-16 -1.02895930e-15  2.32917148e-15\n   4.91719029e-16  8.37878014e-16 -1.20270595e-15 -1.30756693e-15\n  -2.64303667e-15 -1.78909326e-15 -6.05198423e-16  2.71080545e-16\n  -2.04256220e-16 -1.67586818e-15 -1.09106550e-15  8.12328698e-16\n  -1.12123963e-15 -2.45894690e-15  9.02101270e-16  1.04554351e-15\n  -1.70249586e-15 -3.19706746e-15  2.54132864e-15  1.98449497e-15\n   1.42106081e-15 -5.86886011e-16 -1.96188647e-16 -2.04713850e-15\n   2.63365746e-16 -4.13055308e-16 -5.48532490e-16  2.00431191e-15\n  -5.63347283e-16 -1.99423894e-17  1.01726510e-16  9.24638606e-16\n  -1.49049202e-15  1.48467329e-15 -5.33362462e-16  4.43115962e-15\n  -1.29044915e-16 -2.14372710e-15  1.63125403e-15  8.24468789e-16\n  -1.03063646e-15 -1.11827496e-16 -5.22660272e-16  3.23237073e-16\n  -1.46462537e-15  1.75662678e-15  3.32789998e-16  4.79329163e-16\n  -9.16463176e-16 -3.60070555e-16 -7.39863200e-17  5.28598720e-16\n  -1.48533467e-15  1.30087081e-15  3.19273836e-15 -1.18669282e-15\n  -5.21504861e-16  5.76129371e-16 -5.98924028e-16 -7.26279275e-16\n  -1.24756730e-15 -1.20025895e-15 -3.35745051e-16 -7.36080512e-16\n   1.78567369e-16  1.94844343e-15 -1.19174553e-17  1.34481099e-15\n  -1.55360948e-15 -1.34862126e-15  2.35973579e-15 -1.19468110e-15\n   2.11516030e-15 -1.52087741e-15 -4.62517721e-16  4.55345527e-15\n  -6.77508833e-16  1.43334764e-15 -1.73334768e-15 -5.46037799e-16\n   1.61073585e-15  2.53586272e-15  1.05999361e-15  1.68464371e-15\n   3.55125157e-16  2.17078935e-15 -2.93284006e-16  1.40727829e-15\n   5.59841056e-16  1.41436853e-15  3.00619274e-16  5.92805397e-16\n   1.28705999e-16 -1.94793699e-15 -2.20102119e-16  5.75344442e-16\n   3.31832502e-16  1.80966647e-15  2.19348611e-15  1.25738459e-15\n  -3.67049699e-16  3.23520976e-18 -7.95376277e-16  8.62628976e-17\n  -1.14852753e-15 -8.06568463e-16 -8.83366838e-17  1.16743893e-15\n   1.26739661e-15 -6.95484146e-16 -3.10194948e-16  9.32158597e-17\n   1.40641772e-15 -1.19287521e-15  2.49699596e-15  2.25949178e-16\n  -3.26437415e-15  4.69855452e-17 -5.36335571e-16 -1.05106135e-15\n   4.36300721e-15  4.75971389e-16 -3.01180884e-16  1.16509529e-15\n   5.63765190e-16 -1.23658448e-15  4.94080500e-17  4.01698440e-16\n  -1.58831230e-15 -7.23431914e-16 -2.34886710e-15 -3.11820868e-15\n   1.32322613e-15 -9.23444894e-16 -2.69903875e-15  5.08137327e-16\n   3.80392131e-16  1.23916334e-15 -2.54584494e-15 -5.82180144e-16\n   8.65525000e-16 -1.12855469e-15  1.84851202e-15 -1.44318624e-15\n   7.39280344e-16  1.18138674e-15  1.77805985e-16 -2.23901874e-17\n  -1.09781803e-15 -1.44375743e-15  1.33386673e-15  4.46712164e-17\n  -9.86432427e-16 -9.14027277e-17  3.24764747e-16  2.96249056e-15\n   1.01431941e-15 -8.79501520e-16 -2.39623569e-15  6.78258618e-16\n   2.29821532e-15  2.81515005e-16  9.45444954e-16 -3.01200548e-16\n   9.16228440e-16  8.63990230e-17  1.71317099e-15 -3.99614542e-16\n  -1.52572601e-15  9.48633298e-16  6.34115834e-16 -3.11450311e-16\n  -1.65316548e-16  2.78184077e-16 -5.12312201e-16 -2.42845602e-15\n  -3.30353276e-16  1.12550752e-15  1.18161064e-15 -9.83035531e-16\n  -1.95735091e-16 -5.31978307e-16  3.97325406e-15  7.64830221e-17\n  -2.26402527e-15 -1.06067457e-15 -2.09816804e-17  1.38935960e-16\n  -6.16697142e-16  8.86777707e-16 -7.05180113e-16  2.37992714e-15\n  -2.25855920e-16  1.05557841e-15 -1.02399614e-17 -2.07530470e-15\n   1.77042447e-15 -3.45616941e-16 -1.10855289e-15  1.15892262e-17\n   8.74603642e-16  1.73191696e-15 -1.28959067e-16  1.02854619e-16\n   9.24605283e-16  1.44229488e-15  3.26527805e-16  3.75655287e-16\n   5.76942355e-16 -2.39957389e-16  5.63390958e-16  8.70645406e-16\n   9.26192555e-16 -1.30227859e-15 -1.49605548e-15 -5.05094061e-16\n   1.74059821e-16  7.42464330e-16  1.60486950e-15 -6.67900278e-16\n   6.31560573e-16  1.63050354e-15  2.00637981e-15 -3.59169657e-16\n   7.81173748e-16 -1.69561844e-15  5.36581426e-16  7.05140179e-16\n   1.03034313e-15  5.03456953e-16 -6.03512816e-16 -3.10085151e-15\n  -8.16704998e-16 -2.86535390e-15 -7.35953677e-16  1.84768505e-15\n  -3.49131462e-16  9.07742050e-16 -4.11348314e-16 -1.54721639e-15\n   2.29176198e-15 -6.14870964e-16 -1.39890248e-15 -7.59814687e-16\n  -1.62166807e-17 -1.64524997e-15 -3.85301217e-17 -5.26893132e-16\n   6.93743629e-16 -3.04913234e-17  6.79678173e-16 -9.67475865e-16\n   1.23316132e-16 -1.16244106e-15  1.30423693e-16 -1.00296613e-15\n  -2.35961786e-15 -1.31411010e-15  1.94079596e-16 -2.38308126e-15\n  -9.12153512e-17 -6.77450886e-16 -1.62319603e-15 -5.95723179e-16\n   1.72326852e-15 -7.62392526e-16  2.42169988e-15 -8.98612532e-16\n   1.41574554e-15 -6.77734924e-19 -1.31126213e-15 -8.02524804e-16\n  -3.96146887e-16 -1.24438164e-15 -3.47242136e-17 -5.65582641e-16\n   1.54999109e-15  6.27922428e-17 -7.39535114e-16 -4.68044524e-17\n  -9.26018958e-18  1.42763442e-15 -7.97629758e-16  5.91696711e-16\n   1.14659296e-15 -3.24607442e-16  1.87646713e-16 -2.36214851e-15\n   1.43885994e-15 -3.00062797e-16 -5.42354988e-16 -1.48151361e-15\n  -8.96894287e-16  9.98516004e-16 -5.44753500e-16 -6.48776000e-16\n  -1.47783423e-16  1.72245412e-15  7.78419826e-16  1.63478431e-16\n  -1.56943811e-16 -1.48555096e-15  2.01764359e-18 -6.96502248e-16\n  -1.05190551e-15  4.38760429e-16  1.06304465e-15  5.32142667e-16\n  -3.36435828e-16 -1.64796654e-16 -9.16539587e-16  1.75671990e-15\n  -2.15685454e-15  9.65548419e-16 -1.54619560e-15 -2.31899064e-15\n   1.18565915e-15 -9.07000648e-16  1.78945082e-15 -2.68055829e-16\n  -6.83444909e-16  1.35871227e-15  9.08534896e-16 -6.18160513e-16\n   5.64320511e-16 -1.29715515e-16 -1.81821668e-16 -1.13524977e-15\n  -1.40582086e-16  2.75464837e-17 -5.69275326e-16  1.11087791e-15\n   2.75896697e-16  1.03221344e-15  2.18318474e-15  8.07351100e-16\n   1.36058232e-15 -7.22067468e-16  8.74520398e-16  3.40654296e-16\n  -6.43066033e-16  7.07640633e-16  1.57877513e-15 -1.72656158e-15\n  -8.13231664e-16  7.46490146e-16  3.66165118e-16 -2.09268506e-15\n  -5.45219818e-16  1.35728176e-15  1.61026793e-15 -7.74951595e-16\n  -7.10920001e-16 -1.70573880e-16 -5.68673630e-16 -1.27188012e-15\n  -2.08336303e-16 -3.99311255e-17  3.02926878e-16 -7.66276344e-16\n  -1.52030662e-16 -5.52203078e-17  1.96778988e-15 -4.80931488e-16\n   5.82874726e-16  8.98581460e-16 -6.50278200e-16 -5.79310348e-16\n  -1.47845422e-15  8.31324202e-16 -5.80218131e-16 -7.81845338e-16\n   9.29217460e-16  9.27990238e-16 -4.74733100e-16  9.21934723e-18\n   1.79746358e-15 -1.42240680e-15  6.68491400e-16  7.78832428e-17\n  -2.26108641e-16 -1.01298038e-15  1.88382461e-15 -3.00788927e-16\n  -1.48026211e-16 -8.12056040e-16  4.78160975e-17 -1.96631713e-17\n   1.37740264e-15  1.46466504e-15 -1.27078303e-15 -1.06748224e-15\n   1.23946190e-15 -1.44479451e-15 -4.25660266e-16  1.81743384e-15\n  -1.44781714e-15  4.08016752e-16 -4.20775639e-16 -2.50463588e-15\n  -1.89480047e-15 -7.10577609e-17  7.58314395e-16  6.96406537e-17\n  -9.16625959e-16  4.00191091e-15  5.82553852e-16  1.92798830e-15\n  -3.56983540e-16  8.74794695e-16  1.79522118e-15  2.63640593e-15\n   1.20130624e-15  1.22735647e-15 -1.47311962e-15 -2.53164532e-16\n   1.31779396e-15  1.35076454e-15  2.08893809e-16  1.18425800e-15\n   8.26149432e-16  1.12797942e-16 -4.73598769e-16  7.81986306e-16\n  -1.52820211e-15  3.30964925e-15  1.27984327e-15 -4.76514695e-16\n   2.43996123e-15  2.36095643e-16  2.49444653e-15  8.07423127e-18\n  -5.34806221e-16 -5.88221995e-16 -2.03934192e-15  1.54013125e-15\n   1.02932343e-15  5.75947510e-16  3.62794941e-16 -5.00570767e-16\n  -1.34241880e-15 -2.53674920e-15  3.57670878e-16  1.76710882e-15\n   5.61882467e-16 -1.62873798e-16 -9.67800884e-18  1.76101272e-15\n   1.28350908e-15  2.18151870e-15 -6.55603416e-16 -8.16113868e-16\n   7.75475542e-16  1.02425079e-15  1.91946613e-15 -1.53842186e-15\n   6.52400054e-16 -3.36037331e-16  5.74296066e-16 -1.20792795e-15\n   1.55393451e-15  1.34947237e-15  1.37125285e-15 -1.68408592e-15\n  -6.02302421e-16  7.80610216e-16  1.42331141e-15 -1.78562710e-15\n  -1.54914984e-15  4.66507324e-16 -3.02746157e-16  1.31508597e-15\n   7.51039315e-16 -2.12113142e-16  1.61062559e-15  7.95689473e-16\n   2.78001419e-16 -6.90151694e-16 -4.73373583e-16  7.09359188e-16\n   4.10607492e-16  1.19505023e-15  1.92081251e-15 -5.26919972e-16\n   6.08002260e-16  2.86193633e-15 -8.79263212e-16  1.54623066e-16\n  -1.02503577e-15 -1.05584213e-15 -6.31514113e-17  6.20791508e-17\n  -1.77054068e-16  4.70126686e-16  2.13986383e-15 -1.24851482e-16\n  -1.85487566e-16 -1.65296105e-15  2.12824317e-17  1.12309548e-15\n   8.27444515e-16  1.21468826e-15  9.64294163e-16 -7.64620751e-16\n  -1.05352213e-15 -6.65795155e-16  1.35451173e-15 -9.87368871e-16\n   1.77141722e-15  5.91379819e-16  1.49149012e-15 -8.60485817e-16\n  -9.05271369e-16  6.96778468e-16 -5.82779448e-16 -2.82869766e-16\n   2.34360780e-15 -1.56190481e-15  5.99692845e-16  7.10762187e-16\n  -2.75615082e-16  6.15856417e-16  7.31796341e-16  1.46201992e-15\n   1.73250164e-17 -2.96715723e-16 -1.33711018e-15 -2.73104133e-16\n  -1.99169588e-16 -7.74135680e-16 -1.74042728e-16  1.26387492e-15\n  -1.69100150e-16  1.22507197e-17 -9.44613106e-17  5.14643682e-16\n   1.36452830e-15  1.28616171e-15 -1.23335895e-16 -1.09370185e-15\n   1.43932968e-15  9.52499412e-16 -1.18526458e-15 -5.95179850e-16\n  -5.73064864e-16  1.04658021e-16  8.90486890e-16 -1.69280348e-15\n  -2.92655950e-16 -6.91002068e-16 -2.68958587e-16  4.24229450e-16\n   5.02819110e-16 -6.52578343e-16 -1.42586043e-16 -3.27649893e-16\n   2.28638144e-15  1.55335218e-16 -6.60703302e-16 -2.45330488e-15\n   9.55784445e-16 -5.89429564e-16  7.10481310e-16  9.61984833e-16\n   6.07440361e-16  2.85404882e-16 -1.18507280e-15  9.70773852e-16\n   5.54780820e-16 -8.18964543e-16 -8.57414211e-16 -4.04947984e-16\n  -1.83948845e-15 -6.90940122e-16  4.92775227e-16 -9.57666290e-16\n   1.28243029e-15 -1.93024036e-15 -6.42955062e-16 -1.02029936e-15\n   7.96737765e-16 -7.85615527e-16 -1.18106311e-15 -7.96367912e-17\n   2.36718394e-15  1.35805157e-15 -1.00371335e-15 -1.48136854e-15\n   1.47315878e-16 -4.20129543e-16 -7.69528332e-16 -2.76311392e-16\n   6.11695292e-16 -9.18131406e-16  5.50826887e-17  6.84889231e-17\n   2.65634959e-17 -1.58545213e-17  8.78947693e-16 -2.02008664e-16\n  -1.37015676e-15 -1.99080664e-15 -1.02358405e-15 -2.89869965e-15\n   3.04525408e-16 -9.77175434e-16  1.30253764e-15  2.09873479e-16\n  -2.19533411e-15  8.75221417e-17  1.15565170e-15  1.55257836e-15\n   1.64135871e-17  3.09866236e-17  2.08309173e-15 -9.40324404e-16\n  -2.57964035e-15  2.21125894e-15 -1.64635661e-15  1.05822948e-15\n   1.67734580e-15 -7.83107108e-16 -2.24957198e-16 -3.66347179e-16\n  -2.49193952e-16  1.42214822e-15  6.88316632e-16 -1.14926601e-15\n  -1.14056049e-15 -2.15690623e-15 -9.37312218e-16  5.30296154e-16\n   4.90060354e-16  8.74368220e-16  8.61556059e-16  2.62310646e-15\n  -4.60793255e-16 -4.41916006e-16  3.62342571e-16 -4.43636112e-16\n   1.72831604e-15  3.78439575e-16  1.00718935e-15 -8.22921091e-17\n  -1.56287338e-16 -1.41693592e-15  3.66671041e-16 -1.11880568e-15\n   1.47432237e-16 -1.75098601e-15 -7.60656279e-16  1.91007148e-15\n   5.09951055e-16  1.03012314e-15  1.07732316e-15 -1.18377955e-16\n   1.12158289e-15 -8.95029508e-16  4.42557271e-16  4.44992654e-16\n  -1.36009151e-15 -1.35350097e-15 -7.57768143e-18  6.71947082e-16\n   3.97801758e-17  8.75155505e-16  4.35909459e-16  2.42882939e-15\n   2.51529129e-15  1.05085922e-15  2.55137104e-17 -1.04978239e-15\n  -6.04883354e-16  1.99977212e-16  8.53471300e-17 -1.54135296e-16\n  -1.06263370e-15  3.50984802e-16  1.04127188e-15  1.91953593e-16\n  -4.60760262e-16  1.57310331e-15  1.34547841e-15  6.56554702e-16\n   1.55902142e-15  1.64021459e-15  4.60250030e-16  7.49207652e-16\n   2.46732307e-15  4.21259262e-16 -2.30939376e-15 -7.31825464e-16\n  -2.85396931e-16 -5.01944480e-17 -3.68698270e-16 -2.00269650e-15]] [[ 1.59176452e+01  0.00000000e+00  2.49026010e+01 ...  0.00000000e+00\n  -6.11898398e-03  0.00000000e+00]\n [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n  -6.11898398e-03  0.00000000e+00]\n [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n  -6.11898398e-03  0.00000000e+00]\n ...\n [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n  -6.11898398e-03  0.00000000e+00]\n [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n  -6.11898398e-03  0.00000000e+00]\n [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n   1.63425824e+02  0.00000000e+00]]\n\nPCA Applied......................\n\nTraning Model....................\n"
     ]
    }
   ],
   "source": [
    "# query_dataset.shape\n",
    "\n",
    "print(\"Data Processing Completed........\")\n",
    "print(\"\")\n",
    "print(\"Applying PCA.....................\")\n",
    "\n",
    "# test_pca, scaled_data = applyPCA(query_dataset)\n",
    "print(\"In function\")\n",
    "scaler2 = StandardScaler()\n",
    "#scaler=MinMaxScaler()\n",
    "print(\"Standard scaler defined\")\n",
    "scaler2.fit(query_dataset)\n",
    "print(\"scaler Fit defined\")\n",
    "scaled_data_query = scaler2.transform(query_dataset)\n",
    "print(\"scaler trasnform\")\n",
    "\n",
    "# pca_q = PCA(n_components=3000)\n",
    "# pca_q.fit(scaled_data_query)\n",
    "test_pca = pca_q.transform(scaled_data_query)\n",
    "print(\"Data Variance: \", sum(pca_q.explained_variance_ratio_)*100)\n",
    "\n",
    "print(test_pca,scaled_data)\n",
    "print(\"\")\n",
    "print(\"PCA Applied......................\")\n",
    "print(\"\")\n",
    "print(\"Traning Model....................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 900)"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load('model_70acc.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "new_model.predict(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.externals.joblib as extjoblib\n",
    "# import joblib\n",
    "# joblib.dump(clf, 'filename.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test\n",
    "# pca=PCA(n_components=2)\n",
    "# pca.fit(scaled_data)\n",
    "# x_pca=pca.transform(scaled_data)\n",
    "\n",
    "# plt.figure(figsize=(20,12))\n",
    "# plt.scatter(x_pca[:,0],x_pca[:,1])\n",
    "# plt.xlabel('First principle component')\n",
    "# plt.ylabel('Second principle component')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "66cab2f9c265ece95b3064c3649bf2263af44ad45cce960d239f78ecc64793a8"
   }
  },
  "interpreter": {
   "hash": "66cab2f9c265ece95b3064c3649bf2263af44ad45cce960d239f78ecc64793a8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}