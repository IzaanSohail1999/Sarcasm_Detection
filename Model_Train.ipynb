{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assured-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import json\n",
    "import math\n",
    "from nltk.util import pr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "Size = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# functions\n",
    "\n",
    "def make_idf(lexicon):\n",
    "    idf = dict()\n",
    "    global Size\n",
    "    N = Size\n",
    "    for term,docs in lexicon.items():\n",
    "        df = len(docs)\n",
    "        temp = math.log(N / df)\n",
    "        idf[term] = temp\n",
    "    return idf\n",
    "\n",
    "def cleaner(uncleaned):\n",
    "    porter = PorterStemmer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    uncleaned = uncleaned.lower()\n",
    "    uncleaned = uncleaned.strip()\n",
    "    uncleaned = uncleaned.translate(\n",
    "        {ord(i): None for i in '!\\\\@#-_:$%^&*();.,?/1”2’3“4‘567890\\'\\\"'})\n",
    "    for i in uncleaned:\n",
    "        t = ord(i)\n",
    "        if t < 97 or t > 122:\n",
    "            uncleaned = uncleaned.replace(i, \"\")\n",
    "    uncleaned = porter.stem(uncleaned)\n",
    "    uncleaned = lemma.lemmatize(uncleaned, pos='v')\n",
    "    return uncleaned\n",
    "\n",
    "def Read_Data_From_File():\n",
    "    Tokens = []\n",
    "    alpha = ''\n",
    "    Raw_Data = dict()\n",
    "    X_headline = []\n",
    "    Y_Sarcastic = []\n",
    "    i = 0\n",
    "    count = 0\n",
    "    lexicon = {}\n",
    "\n",
    "    stop_word = []\n",
    "    with open(\"Stopword-List.txt\", 'r') as stop:\n",
    "        for line in stop:\n",
    "            temp = line.strip()\n",
    "            stop_word.append(temp)\n",
    "\n",
    "    for lines in open('Sarcasm_Headlines_Dataset.json', 'r'):\n",
    "        count += 1\n",
    "        Raw_Data[i] = json.loads(lines)\n",
    "        X_headline.append(Raw_Data[i][\"headline\"])\n",
    "        Y_Sarcastic.append(Raw_Data[i][\"is_sarcastic\"])\n",
    "\n",
    "        #tokenize here\n",
    "        words = word_tokenize(X_headline[i])\n",
    "        curr = str(i)\n",
    "        for word in words:\n",
    "            if len(word) >= 3:\n",
    "                if word in stop_word:\n",
    "                    continue\n",
    "                taggs = nltk.tag.pos_tag(word)\n",
    "                if(taggs[0][1] != 'NNP' and taggs[0][1] != 'FW' and taggs[0][1] != 'PRP'):\n",
    "                    word = cleaner(word)\n",
    "                    if len(word) >= 3:                 \n",
    "                        # Tokens.append(word)\n",
    "\n",
    "                        if word in lexicon:\n",
    "                            if curr in lexicon[word]:\n",
    "                                lexicon[word][i] += 1\n",
    "                            else:\n",
    "                                lexicon[word][i] = 1\n",
    "                        else:\n",
    "                            lexicon[word] = {}\n",
    "                            if curr in lexicon[word]:\n",
    "                                lexicon[word][i] += 1\n",
    "                            else:\n",
    "                                lexicon[word][i] = 1\n",
    "        i = i+1\n",
    "        # print(lexicon)\n",
    "    global Size \n",
    "    Size = count\n",
    "    return lexicon\n",
    "\n",
    "def tf_idf_lexicon(lexicon, idf):\n",
    "    tf_idf = {}\n",
    "    for term, docs in lexicon.items():\n",
    "        tf_idf[term] = {}\n",
    "        for docNo, tf in docs.items():\n",
    "            tf_idf[term][docNo] = tf * idf[term]\n",
    "\n",
    "    return tf_idf\n",
    "\n",
    "def applyPCA(df):\n",
    "    print(\"In function\")\n",
    "    scaler = StandardScaler()\n",
    "    #scaler=MinMaxScaler()\n",
    "    print(\"Standard scaler defined\")\n",
    "    scaler.fit(df)\n",
    "    print(\"scaler Fit defined\")\n",
    "    scaled_data = scaler.transform(df)\n",
    "    print(\"scaler trasnform\")\n",
    "\n",
    "    pca = PCA(n_components=900)\n",
    "    pca.fit(scaled_data)\n",
    "    x_pca = pca.transform(scaled_data)\n",
    "    print(\"Data Variance: \", sum(pca.explained_variance_ratio_)*100)\n",
    "\n",
    "    return x_pca, scaled_data\n",
    "\n",
    "\n",
    "def sort(tfidf):\n",
    "    size = 26709\n",
    "    list1 = []\n",
    "    check = False\n",
    "    for i in range(size):\n",
    "        list1.append(i+1)\n",
    "    # print(list1)\n",
    "    \n",
    "    for word,doc in tfidf.items():\n",
    "        print(word)\n",
    "        for i in list1:\n",
    "            if i not in doc.keys():\n",
    "                tfidf[word][i] = 0\n",
    "    return tfidf\n",
    "\n",
    "def make_idf1(Tokens):\n",
    "    list1 = []\n",
    "    idf = dict()\n",
    "    global Size\n",
    "    N = Size\n",
    "    # for term,docs in lexicon.items():\n",
    "    #     df = len(docs)\n",
    "    #     temp = math.log(N / df)\n",
    "    #     idf[term] = temp\n",
    "    # return idf\n",
    "    for i in Tokens:\n",
    "        # print(i[0],i[1])        \n",
    "        count =  i[0]\n",
    "        while i[0]+1 == count:\n",
    "             list1.append(Tokens[i[1]])\n",
    "    print(list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(26709, 18622)\n",
      "18622\n",
      "Data Processing Completed........\n",
      "\n",
      "Applying PCA.....................\n",
      "In function\n",
      "Standard scaler defined\n",
      "scaler Fit defined\n",
      "scaler trasnform\n",
      "Data Variance:  22.363574441852904\n",
      "[[-5.38722640e-02 -3.07770974e-02  4.58026224e-03 ... -5.18182851e-01\n",
      "   1.66149708e+00 -2.00327479e-01]\n",
      " [-3.97918282e-02 -1.44887513e-02 -3.78601933e-02 ... -3.32642575e-01\n",
      "  -1.82867024e-01 -1.56176632e-02]\n",
      " [-8.34543482e-02 -4.57898260e-02 -2.62328096e-02 ...  5.92836469e+00\n",
      "   1.38415095e+00  2.00209451e-01]\n",
      " ...\n",
      " [-3.34488689e-02 -1.00313559e-02 -3.62179674e-02 ... -2.10659051e-01\n",
      "   1.33084296e-01 -6.56137467e-02]\n",
      " [-6.23458095e-02 -1.98454765e-02  1.07939651e-02 ... -2.23850604e+00\n",
      "  -5.17559331e-02 -1.62762250e+00]\n",
      " [-3.02392648e-02 -6.45818761e-03 -3.72011666e-02 ...  5.25561592e-02\n",
      "   1.86405192e-01  1.95317082e-02]] [[ 1.59176452e+01  0.00000000e+00  2.49026010e+01 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " ...\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "  -6.11898398e-03  0.00000000e+00]\n",
      " [-6.28233629e-02  0.00000000e+00 -4.01564480e-02 ...  0.00000000e+00\n",
      "   1.63425824e+02  0.00000000e+00]]\n",
      "\n",
      "PCA Applied......................\n",
      "\n",
      "Traning Model....................\n"
     ]
    }
   ],
   "source": [
    "# lexicon = Read_Data_From_File()\n",
    "# print(lexicon)\n",
    "# idf = make_idf(lexicon)\n",
    "# print(\"Idf made\")\n",
    "# tf_idf = tf_idf_lexicon(lexicon,idf)\n",
    "# print(\"tf_Idf made\")\n",
    "# # tf_idf = sort(tf_idf)\n",
    "# with open(\"TFIDF_Dictionary.json\", \"w\") as f: # Writing the index to JSON File.\n",
    "#    f.write(json.dumps(tf_idf, sort_keys=False, indent=4)) \n",
    "with open('TFIDF_Dictionary.json') as json_file:\n",
    "    Dict = json.load(json_file)\n",
    "# Dict = sort(Dict)\n",
    "# print(Dict)\n",
    "# with open(\"TFIDF_Dictionary_new.json\", \"w\") as f:\n",
    "    # f.write(json.dumps(Dict, sort_keys=False, indent=4))\n",
    "\n",
    "#Create Panda DataFrame\n",
    "#    print(\"Hello\")\n",
    "df = pd.DataFrame(Dict.items())\n",
    "vocab = list(Dict.keys())\n",
    "vocab_len = len(vocab)\n",
    "word_to_inx = {}\n",
    "for word in vocab:\n",
    "    word_to_inx[word] = vocab.index(word)\n",
    "i = 0\n",
    "dataset = np.empty((26709, vocab_len))\n",
    "for line in open('Sarcasm_Headlines_Dataset.json', 'r'):\n",
    "    data = json.loads(line)\n",
    "    str_i = str(i)\n",
    "    docVector = np.zeros(vocab_len)\n",
    "    for word in data[\"headline\"].split(\" \"):\n",
    "        if word in word_to_inx and str_i in Dict[word]:\n",
    "            wordPos = word_to_inx[word]\n",
    "            docVector[wordPos] = Dict[word][str_i]\n",
    "            dataset[i] = docVector\n",
    "    i += 1\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "# df = np.transpose(df)\n",
    "# print(df[0])\n",
    "\n",
    "print(vocab_len)\n",
    "\n",
    "print(\"Data Processing Completed........\")\n",
    "print(\"\")\n",
    "print(\"Applying PCA.....................\")\n",
    "\n",
    "x_pca, scaled_data = applyPCA(dataset)\n",
    "\n",
    "print(x_pca,scaled_data)\n",
    "print(\"\")\n",
    "print(\"PCA Applied......................\")\n",
    "print(\"\")\n",
    "print(\"Traning Model....................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "Raw_Data = dict()\n",
    "X_headline = []\n",
    "Y_Sarcastic = []\n",
    "i = 0\n",
    "count = 0\n",
    "\n",
    "for lines in open('Sarcasm_Headlines_Dataset.json', 'r'):\n",
    "    count += 1\n",
    "    Raw_Data[i] = json.loads(lines)\n",
    "    X_headline.append(Raw_Data[i][\"headline\"])\n",
    "    Y_Sarcastic.append(Raw_Data[i][\"is_sarcastic\"])\n",
    "\n",
    "size=len(X_headline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26709"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(Y_Sarcastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Applying SVM.....................\n"
     ]
    }
   ],
   "source": [
    "#x_pca.to_csv('pData.csv')\n",
    "#x_pca=pd.read_csv('pData')\n",
    "\n",
    "#BreakData\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_pca, Y_Sarcastic[:size],test_size=0.20,random_state=48)\n",
    "\n",
    "print(\"Applying SVM.....................\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Trained....................\n\nApplying Model...................\n"
     ]
    }
   ],
   "source": [
    "#Apply SVM\n",
    "model = svm.SVC(random_state=20)\n",
    "print(\"Model Trained....................\")\n",
    "print(\"\")\n",
    "print(\"Applying Model...................\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(random_state=20)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nAccuracy=  70.47922126544366 %\n"
     ]
    }
   ],
   "source": [
    "acc=model.score(X_test,y_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy= \",acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.externals.joblib'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-351b8cedde3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoblib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mextjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'filename.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.externals.joblib'"
     ]
    }
   ],
   "source": [
    "import sklearn.externals.joblib as extjoblib\n",
    "import joblib\n",
    "joblib.dump(clf, 'filename.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test\n",
    "# pca=PCA(n_components=2)\n",
    "# pca.fit(scaled_data)\n",
    "# x_pca=pca.transform(scaled_data)\n",
    "\n",
    "# plt.figure(figsize=(20,12))\n",
    "# plt.scatter(x_pca[:,0],x_pca[:,1])\n",
    "# plt.xlabel('First principle component')\n",
    "# plt.ylabel('Second principle component')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "66cab2f9c265ece95b3064c3649bf2263af44ad45cce960d239f78ecc64793a8"
   }
  },
  "interpreter": {
   "hash": "66cab2f9c265ece95b3064c3649bf2263af44ad45cce960d239f78ecc64793a8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}